services:
  asr:
    build:
      context: ..
      dockerfile: docker/asr-fast.Dockerfile
    image: psyched-asr-fast:local
    restart: unless-stopped
    ports:
      FOREBRAIN_LLM__MODEL__PATH: "/data/gpt-oss-20b-Q5_K_M.gguf"

  tts:
    build:
      context: ..
      dockerfile: docker/tts-websocket.Dockerfile
    image: psyched-tts-websocket:local
    restart: unless-stopped
    environment:
      TTS_MODEL: "tts_models/en/vctk/vits"
      # Run on CPU by default; set to 'true' to try enabling CUDA (requires CUDA-enabled PyTorch in the image)
      TTS_USE_CUDA: "false"
      # Directory inside the container with locally-provided TTS assets
      TTS_MODEL_DIR: "/models"
    ports:
      - "5002:5002"
    volumes:
      - type: bind
        source: ../tools/tts_websocket/models
        target: /models
        read_only: false

  llm:
    build:
      context: ..
      dockerfile: docker/forebrain-llm.Dockerfile
    image: psyched-forebrain-llm:local
    restart: unless-stopped
    environment:
      FOREBRAIN_LLM__MODEL__PATH: "/data/gpt-oss-20b-Q5_K_M.gguf"
      FOREBRAIN_LLM__WEBSOCKET__BIND_ADDR: "0.0.0.0:8080"
      NVIDIA_VISIBLE_DEVICES: all
    volumes:
      - type: bind
        source: ../forebrain-llm/models
        target: /data
        read_only: false
    # Backwards-compatible GPU access for older docker-compose validators:
    # expose common NVIDIA device nodes. Modern Docker + nvidia-container-toolkit
    # does not require a special 'nvidia' runtime and works with the /dev mounts.
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia-uvm:/dev/nvidia-uvm
      - /dev/nvidia-modeset:/dev/nvidia-modeset
    ports:
      - "8080:8080"
